{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dfc592a-1dd6-46e3-9ea4-3cf5366ba1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 1: DEFINIÇÃO DE PARÂMETROS E PREPARAÇÃO\n",
    "# ==============================================================================\n",
    "# Este notebook é projetado para ser executado como um job do Databricks.\n",
    "\n",
    "# 1. Define os widgets que serão preenchidos pelo job do Databricks.\n",
    "print(\"1. Definindo widgets de entrada...\")\n",
    "dbutils.widgets.text(\"catalog\", \"\", \"Catálogo do Unity Catalog\")\n",
    "dbutils.widgets.text(\"schema\", \"\", \"Schema de Destino\")\n",
    "dbutils.widgets.text(\"folder_volume\", \"\", \"Pasta no Volume que contém o arquivo Parquet\")\n",
    "dbutils.widgets.text(\"file_parquet\", \"\", \"Nome do Arquivo Parquet\")\n",
    "dbutils.widgets.text(\"table_name\", \"\", \"Nome Base da Tabela Delta (vindo do nome da pasta)\")\n",
    "# A pasta de destino não é mais necessária, pois a tabela será gerenciada\n",
    "# dbutils.widgets.text(\"delta_folder\", \"api_deltatables\", \"Pasta de Destino para Tabelas Delta\")\n",
    "print(\"   - Widgets definidos com sucesso.\")\n",
    "\n",
    "# 2. Lê os valores dos widgets para variáveis Python.\n",
    "print(\"\\n2. Lendo parâmetros do job...\")\n",
    "catalog_name = dbutils.widgets.get(\"catalog\")\n",
    "schema_name = dbutils.widgets.get(\"schema\")\n",
    "folder_path_in_volume = dbutils.widgets.get(\"folder_volume\")\n",
    "parquet_filename = dbutils.widgets.get(\"file_parquet\")\n",
    "delta_table_name_raw = dbutils.widgets.get(\"table_name\")\n",
    "\n",
    "# Define o nome do volume base.\n",
    "base_volume_name = \"raw\"\n",
    "\n",
    "# 3. Constrói os caminhos e nomes de tabela.\n",
    "print(\"\\n3. Construindo nomes e caminhos...\")\n",
    "full_parquet_path = f\"/Volumes/{catalog_name}/{schema_name}/{base_volume_name}/{folder_path_in_volume}/{parquet_filename}\"\n",
    "\n",
    "# CORREÇÃO APLICADA AQUI: O nome da tabela agora é apenas o nome do endpoint.\n",
    "final_table_name = delta_table_name_raw.lower().replace('-', '_')\n",
    "full_table_name = f\"{catalog_name}.{schema_name}.{final_table_name}\"\n",
    "\n",
    "# Imprime as variáveis para verificação no log do job.\n",
    "print(f\"   - Arquivo de Origem: {full_parquet_path}\")\n",
    "print(f\"   - Tabela de Destino Final: {full_table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34e1fa03-5c8a-4c09-94ed-36c171ee8906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CÉLULA 2: LÓGICA DE CRIAÇÃO E CARGA\n",
    "# ==============================================================================\n",
    "\n",
    "# 4. Lê o arquivo Parquet para obter seu schema dinamicamente.\n",
    "print(\"\\n4. Lendo o schema do arquivo Parquet de origem...\")\n",
    "try:\n",
    "    source_df = spark.read.parquet(full_parquet_path)\n",
    "    # Converte o schema do DataFrame para uma string no formato DDL (ex: \"`col1` STRING, `col2` INT\").\n",
    "    schema_ddl = \", \".join([f\"`{field.name}` {field.dataType.simpleString()}\" for field in source_df.schema.fields])\n",
    "    print(\"   - Schema DDL gerado com sucesso.\")\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"ERRO: Falha ao ler o arquivo Parquet para inferir o schema. Caminho: {full_parquet_path}. Erro: {e}\")\n",
    "\n",
    "# 5. Apaga a tabela antiga (se existir) e a recria com o novo schema.\n",
    "# Esta abordagem de \"overwrite\" completo é simples e garante que a tabela reflita o arquivo mais recente.\n",
    "print(f\"\\n5. Criando/Recriando a tabela '{full_table_name}'...\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {full_table_name} ({schema_ddl})\n",
    "    USING DELTA\n",
    "\"\"\") # Cria como uma tabela gerenciada, a melhor prática.\n",
    "print(\"   - Tabela criada com sucesso.\")\n",
    "\n",
    "# 6. Carrega os dados na tabela recém-criada usando COPY INTO.\n",
    "print(f\"\\n6. Carregando dados com COPY INTO...\")\n",
    "copy_into_sql = f\"\"\"\n",
    "    COPY INTO {full_table_name}\n",
    "    FROM '{full_parquet_path}'\n",
    "    FILEFORMAT = PARQUET\n",
    "    COPY_OPTIONS (\n",
    "      'force' = 'true',            -- Força o carregamento, já que a tabela está sempre vazia neste ponto.\n",
    "      'mergeSchema' = 'true'       -- Boa prática para permitir que novas colunas sejam adicionadas no futuro.\n",
    "    )\n",
    "\"\"\"\n",
    "try:\n",
    "    result_df = spark.sql(copy_into_sql)\n",
    "    print(\"   - Carga de dados concluída.\")\n",
    "    display(result_df) # Mostra um resumo da operação de cópia.\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"ERRO ao executar o COPY INTO. Erro: {e}\")\n",
    "\n",
    "# 7. Verificação Final.\n",
    "print(f\"\\n7. Verificando o resultado final...\")\n",
    "try:\n",
    "    record_count = spark.sql(f\"SELECT COUNT(*) FROM {full_table_name}\").collect()[0][0]\n",
    "    print(f\"   - Sucesso! A tabela '{full_table_name}' agora contém {record_count} registros.\")\n",
    "except Exception as e:\n",
    "    dbutils.notebook.exit(f\"ERRO ao verificar a contagem de registros na tabela final. Erro: {e}\")\n",
    "\n",
    "dbutils.notebook.exit(\"Processo finalizado com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "parquet_to_deltatable",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}